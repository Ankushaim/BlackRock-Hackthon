# BlackRock Micro-Savings Challenge

This repository contains the backend APIs for an automated retirement savings system.

It's implemented in **Python 3.13** using **FastAPI** to keep it performant and easy to scale. I also used Pydantic for validation.

---

## A Note on Development Approach

> This project was built under a time constraint as part of a coding challenge.
>
> I used **GitHub Copilot** as a productivity aid during development — similar to how a developer would use Stack Overflow or documentation — to help write boilerplate faster. However, **all architectural decisions, design choices, and component selections were made by me**:
>
> - The choice of FastAPI, `uv`, `ruff`, and `ty` as the toolchain
> - The pipeline-based API design (parse → validate → filter → returns)
> - The temporal rules engine and its edge-case handling (q period override priority, p period accumulation)
> - The decision to use `BackgroundTasks` instead of Celery/Redis for the async enhancement
> - The orchestrator `/calculate` endpoint as a bonus beyond the spec
>
> The LLM helped me type faster. The thinking, reasoning, and trade-offs were mine.
> No code was copied from the internet or any external repository.

---

## Prerequisites

- **Docker** and **Docker Compose** installed on your system.
- Or, if running locally without Docker, [uv](https://docs.astral.sh/uv/) installed on your machine.

## How to Build & Run (Docker)

As specified in the requirements, the system uses a Docker container running on port `5477`. The `Dockerfile` uses a slim python image to keep constraints small.

1. **Build the image**:

   ```bash
   docker build -t blk-hacking-ind-ankush-agrawal .
   ```

2. **Run the container**:
   ```bash
   docker run -d -p 5477:5477 blk-hacking-ind-ankush-agrawal
   ```

Alternatively, you can just use **Docker Compose**:

```bash
docker compose up --build
```

The APIs will now be live at `http://localhost:5477`. You can browse interactive API docs automatically generated by FastAPI at `http://localhost:5477/docs`.

## Local Development (Without Docker)

You can also run it locally using `uv` if you prefer to skip docker:

1. Install dependencies & run:

   ```bash
   uv sync
   uv run uvicorn app.main:app --host 0.0.0.0 --port 5477 --reload
   ```

2. Automatic code tooling:
   ```bash
   uv run ruff check . --fix  # Linting
   uv run ty check .          # Strict Static Typing
   ```

## Running the Tests

Tests are kept in the `test/` folder. They cover the basic bounds, tax slab thresholds, and compound interest math overrides.

To run tests (which are also packaged into the virtual environment), simply run:

```bash
PYTHONPATH=. uv run pytest test/
```

## Endpoints Summary

- `POST /blackrock/challenge/v1/transactions:parse`
  Converts raw expenses to remanents leveraging base 100 ceilings.
- `POST /blackrock/challenge/v1/transactions:validator`
  Screens transactions for logical failures (negatives, massive outliers exceeding ₹5L limits) and squashes duplicate date anomalies.
- `POST /blackrock/challenge/v1/transactions:filter`
  Applies `q` and `p` temporal constraints over transaction timelines.
- `POST /blackrock/challenge/v1/returns:nps` / `returns:index`
  Calculates group aggregation periods `k`, computing future projections dynamically adapting interest returns vs respective tax subsidies.
- `GET /blackrock/challenge/v1/performance`
  Reports system thread execution counts, physical machine RSS memory footprints, and global process timing.
- `POST /blackrock/challenge/v1/calculate` _(bonus orchestrator)_
  Single entry-point that runs the full pipeline end-to-end — parse → validate → filter → returns — and gives back both NPS and Index Fund results in one shot alongside a `summary` of how many transactions were parsed, passed, or rejected at each stage.

## Pipeline Flow

The individual APIs follow a deliberate pipeline. Each step's output feeds the next:

```
Raw Expenses
    │
    ▼
1. /transactions:parse       → ceiling + remanent per expense
    │
    ▼
2. /transactions:validator   → drop negatives, duplicates, out-of-bounds
    │
    ▼
3. /transactions:filter      → apply q (fixed override) and p (extra) period rules
    │
    ▼
4. /returns:nps              → compound interest + tax benefit + inflation
   /returns:index            → compound interest + inflation (no tax benefit)
```

The `/calculate` endpoint runs all four steps internally and returns both NPS and Index results in a single response.

## Architectural Enhancements

### 1. Asynchronous API Processing (The Async/Worker Pattern)

**The Problem:** Passing up to a million transactions synchronously via HTTP will likely cause timeout errors before the server can calculate everything.

**The Solution:** I used **FastAPI BackgroundTasks** to implement a basic async worker setup!

- Instead of blocking the `/returns:nps` endpoint, you can throw massive payloads at `/returns:nps:async`.
- It instantly returns an `HTTP 202` with a `job_id`.
- Calculations happen in the background, and the frontend can just poll `/jobs/{job_id}` to get the final `ReturnsResponse`.
- This is done directly in the python app loop, so we didn't have to add heavier things like celery or redis just to solve timeouts!
